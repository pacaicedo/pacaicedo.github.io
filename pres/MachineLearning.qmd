---
title: "Machine Learning"
description: "Sitio de la asignatura machine learning en la UniAutonoma del Cauca"
subtitle: "Ingeniería de software y computación"
lang: es
author: "Ph.D. Pablo Eduardo Caicedo Rodríguez"
date: "2023-08-01"
format:
  revealjs: 
    self-contained: true
    code-tools: true
    code-overflow: scroll
    code-line-numbers: true
    code-copy: false
    fig-align: center
    theme: 
      - white
      - custom.scss
    slide-number: true
    preview-links: auto
    logo: images/medes.png
    css: styles.css
    footer: <https://pacaicedo.github.io>
    transition: fade
resources:
  - demo.pdf
---

```{python}
#| echo: false
path_to_data = "D:/DatasetsVault" # For Windows
#path_to_data = "/Users/pacaicedo/DatasetsVault" #For MacOsX
```

# Presentación

## El Profesor {.smaller}

::: columns

:::: column


### Educación

Doctor en Ciencias de la Electrónica.
Magíster en Ingeniería Electrónica y Telecomunicaciones
Ingeniero en Electrónica y Telecomunicaciones

### Intereses

Biomecánica, Dispositivos para el análisis de movimiento humano, ciencia de los datos.

::::

:::: column

### Desempeño

Profesor de la Facultad de Ingeniería & Ciencias Naturales

Invest. Línea de Percep. Avanz. y Robótica -- GITA

Director Grupo de Investigación MEDES.

Director del laboratorio de datos de la Uniautonoma.

::::

:::

### Contacto:

pablo.caicedo.r@uniautonoma.edu.co


## Contenido del curso

:::columns

:::: column

![](./images/MachineLearning/wordcloud.png)

::::

:::: column

1. Análisis exploratorio de datos
2. Problemas de regresión
3. Tópicos avanzados en clasificación

::::

:::

## Evaluación

:::columns

:::: column

\scriptsize

1. Comprensión de lectura (Inglés) (10%)
2. Consigna 001. Análisis exploratorio de datos (25%)
2. Consigna 002. Problemas de Regresión (25%)
3. Consigna 002. Proyecto Final (40%)

\normalsize

::::

:::: column

![](./images/MachineLearning/evaluacion.jpg)

::::

:::


## Recursos

### Clases

Lunes, Martes, Jueves y Viernes
11:00 -- 13:00
Sala 504

[Sala de teams](https://teams.microsoft.com/l/channel/19%3anBsyoF3Pd0dMhf7u5Thd9rXN9LMEg_yMNbZY50ACBug1%40thread.tacv2/General?groupId=2587477d-28c8-467d-a1ec-68b4aede62dd&tenantId=c91fc68b-7571-4551-933c-467a6c58bf58)


### Software

**Interpretes:** Python, R, Latex(TEXLive), Anaconda.

**IDE:** Visual Studio Code, Google Colaboratory ([R](https://colab.to/r), [Python](https://colab.to))

**Librerías** Pandas, Matplotlib, Seaborn, Keras, Tensorflow, Numpy, SciKit-Learn, SciPy

**Seguimiento de Aprendizaje:** Moodle


## Bibliografía{.scrollable}


1. B. Boehmke y B. M. Greenwell, Hands-on machine learning with R. Boca Raton: CRC Press, 2019.

2. G. Bonaccorso, Mastering machine learning algorithms: expert techniques to implement popular machine learning algorithms and fine-tune your models. 2018.

3. M. Fenner, Machine learning in python for everyone. Boston, MA: Addison-Wesley, 2019.

4. K. Kolodiazhnyi, Hands-On Machine Learning with C++ Build, Train, and Deploy End-To-end Machine Learning and Deep Learning Pipelines. Birmingham: Packt Publishing, Limited, 2020. Accedido: 28 de septiembre de 2021.

5. M. Kubat, An Introduction to Machine Learning. Cham: Springer International Publishing, 2017. doi: 10.1007/978-3-319-63913-0.

6. S. Raschka y V. Mirjalili, Python machine learning: machine learning and deep learning with Python, scikit-learn, and TensorFlow, Second edition, Fourth release,[fully revised and Updated]. Birmingham Mumbai: Packt Publishing, 04.

7. S. Skansi, Introduction to Deep Learning: From Logical Calculus to Artificial Intelligence. Cham: Springer International Publishing, 2018. doi: 10.1007/978-3-319-73004-2.


# Análisis exploratorio de datos


## A little reminder ...

![](./images/MachineLearning/MLWorkflow.png)

## A little reminder ...

![](./images/MachineLearning/DataEngineer.png)

## A little reminder ...

![](./images/MachineLearning/DataScientist.png)

## A little reminder ...

![](./images/MachineLearning/MLEngineer.png)

## A little reminder ...

![](./images/MachineLearning/MAD_General_ENG.png)

## Exploratory Data Analysis (EDA)

### Definition

The art of looking the underlying structure of the information through one or more datasets.

### Definition by Diaconis, P.

We look at numbers or graphs and try to ﬁnd patterns. We pursue leads suggested by background information, imagination, patterns perceived, and experience with other data analyses.

## Exploratory Data Analysis (EDA)

EDA, depends on two things:

1. Type of variable scale (information type, categorical, numerical, continuous, discrete, etc).

2. Objective and type of the analysis (graphical, numerical, correlation, etc)

## Learning from the example

### Spotify

"Spotify offers digital copyright restricted recorded music and podcasts, including more than 82 million songs, from record labels and media companies" from [wikipedia](https://en.wikipedia.org/wiki/Spotify)

### Spotify Dataset

The data set is located in the kaggle site. [Dataset](https://www.kaggle.com/datasets/yamaerenay/spotify-dataset-19212020-600k-tracks)

## Learning from the example

### Analysis Objective

Spotify wants to know if there is a relationship between the popularity of a song and the number of followers of its singers. The above to generate strategies to attract new singers to the platform.

## Learning from the examples

### General Workflow

1. Import dataset.
2. Preprocessing dataset.
3. EDA on the datasets.
4. Train the machine learning model.
5. Predict the target using the trained model.

## Import dataset.{.scrollable}

### General workflow for Importing a dataset in python

1. Install conda environment manager
2. Install a suitable conda environment.
3. Install python libraries. At least, a machine learning project without deployment needs:

   1. Numpy
   2. Pandas
   3. Matplotlib
   4. Seaborn
   5. Scikit-Learn
   6. Jupyter

4. Install a suitable IDE software.
5. Script, script, script.

## Import dataset{.scrollable}

```{python carga_datos_spotify1}
#| echo: true
#| eval: true
#| output: true
import pandas as pd
data_spotify = pd.read_csv(path_to_data + "/spotify/tracks.csv")
``` 

:::{.formateador_ouput}
```{python}
#| echo: true
#| eval: true
#| output: true

data_spotify.head(5)

```


:::

## Preprocessing dataset.

### General workflow for Preprocessing

1. Check the digital structure of the dataset
2. Check for digital information type
3. Check for nan
4. Check for outlier

## Preprocessing dataset.

:::{.formateador_output}

```{python}
#| echo: true
#| eval: true
#| output: true
type(data_spotify)
```


:::

:::{.formateador_output}

```{python}
#| echo: true
#| eval: true
#| output: true

data_spotify.shape

```


:::

:::{.formateador_output}

```{python}
#| echo: true
#| eval: true
#| output: true

data_spotify.size

```

:::

## Preprocessing dataset.{.scrollable}


:::{.formateador_output}

```{python}
#| echo: true
#| eval: true
#| output: true

data_spotify.describe()
```

:::

# Problemas de regresión

## Linear Regression, python preparation

```{python}
#| echo: true
#| eval: true
#| output: true

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
archivoData = path_to_data+"/Salario/data_for_class.csv"
data = pd.read_csv(archivoData,delimiter=",", decimal=".")

```

## Salary vs Years of experience scatter plot



```{python}
#| echo: true
#| eval: true
#| output: false

ax1 = sns.scatterplot(data=data, x="YearsExperience", y="Salary")
ax1.set(xlabel='x = YearsExperience', ylabel='y = Salary')
plt.show()

```



## Salary vs Years of experience scatter plot



```{python}
#| echo: false
#| eval: true
#| output: true

ax1 = sns.scatterplot(data=data, x="YearsExperience", y="Salary")
ax1.set(xlabel='x = YearsExperience', ylabel='y = Salary')
plt.show()

```

## Salary histogram

```{python}
#| echo: true
#| eval: true
#| output: false

ax2 = sns.displot(data=data, x="Salary")
plt.show()
```

## Salary histogram

```{python}
#| echo: false
#| eval: true
#| output: true

ax2 = sns.displot(data=data, x="Salary")
plt.show()

```

## Linear Regression

```{python}
#| echo: false
#| eval: true
#| output: true

sns.scatterplot(data=data, x="YearsExperience", y="Salary", color='red')
ax2 = sns.lineplot(data=data, x='YearsExperience', y='estimated_y', linestyle='dashed')
ax2.set(xlabel='x = YearsExperience', ylabel='y = Salary')
plt.plot([data.YearsExperience[17], data.YearsExperience[17]], [data.Salary[17], data.estimated_y[17]], linewidth=2, color='green', linestyle='dashed')

```

## Linear Regression

In the example, in previous slide, data was modelled as a linear function. The difference (error) between the modelled data $\left( \hat{y}_n \right)$ and actual data $\left( y_n \right)$ can be written as

::: {.callout-warning title="Cost function" collapsible="false"}

$$E = \frac{1}{N} \sum_{n=1}^{N}{\left( \hat{y}_n - y_n \right)^2}$$

:::


## Some other examples of cost function

$$E = \sqrt{\frac{1}{N} \sum_{n=1}^{N}{\left( \hat{y}_n - y_n \right)^2}}$$

$$E = \frac{1}{N} \sum_{n=1}^{N}{\left| \hat{y}_n - y_n \right| }$$

## Gradient Descent algorithm

Looking the cost surface, we notices that this surface has a global minimum. If we could have an algorithm which automatically finds it.

![Cost Surface](./Images/MachineLearning/CostSurface.png)

## Gradient Descent algorithm

Indeed, there are multiples algorithms for minima searching. The most famous is the one named as _least squares_ but in this course we will use the _gradient descent algorithm._

Assuming that the data model is a function $f\left(\theta_i, x_n, y_n\right)$, where $\theta$ is known as model parameter.

::: {.callout-important title="The gradient descent algorithm" collapsible="false"}

$$\boldsymbol{\theta}_{i,j+1} =  \boldsymbol{\theta}_{i,j} - \eta \frac{\partial E}{\partial \boldsymbol{\theta}_{i}}$$
  



:::


## Gradient Descent algorithm

::: {.callout-warning title="Assumptions" collapsible="false"}

- Linear model for the Regression 
- Mean square error as cost function
- $\eta = 1$

:::

$$\boldsymbol{\theta}_i = \left[ \theta_1, \theta_0 \right]^T$$

$$\hat{y}_n  = \theta_1 x_n + \theta_0$$

$$E = \frac{1}{N} \sum_{n=1}^{N}{\left( \theta_1 x_n + \theta_0 - y_n \right)^2}$$


## Gradient Descent algorithm

:::{.small_font}

::: {.callout-note title="For $\theta_1$ estimation" collapsible="false"}
$$\boldsymbol{\theta}_{1,j+1} = \boldsymbol{\theta}_{1,j} - \eta \frac{\partial E}{\partial \boldsymbol{\theta}_{1}}$$

$$\frac{\partial E}{\partial \boldsymbol{\theta}_{1}} = \frac{\partial}{\partial \boldsymbol{\theta}_{1}} \left( \frac{1}{N} \sum_{n=1}^{N}{\left( \theta_1 x_n + \theta_0 - y_n \right)^2} \right) $$

$$\frac{\partial E}{\partial \boldsymbol{\theta}_{1}}= \frac{1}{N} \frac{\partial}{\partial \boldsymbol{\theta}_{1}} \left(  \sum_{n=1}^{N}{\left( \theta_1 x_n + \theta_0 - y_n \right)^2} \right) $$

$$\frac{\partial E}{\partial \boldsymbol{\theta}_{1}}= \frac{1}{N}  \sum_{n=1}^{N}{\frac{\partial}{\partial \boldsymbol{\theta}_{1}} \left( \left( \theta_1 x_n + \theta_0 - y_n \right)^2\right)}$$

$$\frac{\partial E}{\partial \boldsymbol{\theta}_{1}}= \frac{1}{N}  \sum_{n=1}^{N}{2 \left( \theta_1 x_n + \theta_0 - y_n \right) x_n}$$

:::

:::

## Gradient Descent algorithm

:::{.small_font}

::: {.callout-note title="For $\theta_0$ estimation" collapsible="false"}
$$\boldsymbol{\theta}_{0,j+1} = \boldsymbol{\theta}_{0,j} - \eta \frac{\partial E}{\partial \boldsymbol{\theta}_{1}}$$

$$\frac{\partial E}{\partial \boldsymbol{\theta}_{0}} = \frac{\partial}{\partial \boldsymbol{\theta}_{0}} \left( \frac{1}{N} \sum_{n=1}^{N}{\left( \theta_1 x_n + \theta_0 - y_n \right)^2} \right) $$

$$\frac{\partial E}{\partial \boldsymbol{\theta}_{0}}= \frac{1}{N} \frac{\partial}{\partial \boldsymbol{\theta}_{0}} \left(  \sum_{n=1}^{N}{\left( \theta_1 x_n + \theta_0 - y_n \right)^2} \right) $$

$$\frac{\partial E}{\partial \boldsymbol{\theta}_{0}}= \frac{1}{N}  \sum_{n=1}^{N}{\frac{\partial}{\partial \boldsymbol{\theta}_{0}} \left( \left( \theta_1 x_n + \theta_0 - y_n \right)^2\right)}$$

$$\frac{\partial E}{\partial \boldsymbol{\theta}_{0}}= \frac{1}{N}  \sum_{n=1}^{N}{2 \left( \theta_1 x_n + \theta_0 - y_n \right)}$$

:::

:::

## Changing the cost function and the data model

$$
 \begin{eqnarray}
  E & = & \frac{1}{N} \sqrt{u}\\
  \frac{\partial E}{\partial \boldsymbol{\theta}_{0}} &=& \frac{1}{2 N \sqrt{u}} \frac{\partial u}{\partial \boldsymbol{\theta}_{0}}\\
  \frac{\partial u}{\partial \boldsymbol{\theta}_{0}} &=& 2\sum_{n=1}^{N}{\left( \theta_2 x_{n}^{2} + \theta_1 x_n + \theta_0 - y_n \right)}\\
  \frac{\partial E}{\partial \boldsymbol{\theta}_{0}} &=& \frac{2\sum_{n=1}^{N}{\left( \theta_2 x_{n}^{2} + \theta_1 x_n + \theta_0 - y_n \right)}}{2 N \sqrt{u}}
 \end{eqnarray}
$$

## Changing the cost function and the data model


$$
\begin{eqnarray}
  \frac{\partial E}{\partial \boldsymbol{\theta}_{0}} &=& \frac{\sum_{n=1}^{N}{\left( \theta_2 x_{n}^{2} + \theta_1 x_n + \theta_0 - y_n \right)}}{N \sqrt{\sum_{n=1}^{N}{\left( \theta_2 x_{n}^{2} + \theta_1 x_n + \theta_0 - y_n \right)^2}}}\\
  \frac{\partial E}{\partial \boldsymbol{\theta}_{1}} &=& \frac{\sum_{n=1}^{N}{x_n \left( \theta_2 x_{n}^{2} + \theta_1 x_n + \theta_0 - y_n \right)}}{N \sqrt{\sum_{n=1}^{N}{\left( \theta_2 x_{n}^{2} + \theta_1 x_n + \theta_0 - y_n \right)^2}}}\\
  \frac{\partial E}{\partial \boldsymbol{\theta}_{2}} &=& \frac{\sum_{n=1}^{N}{x_n^2 \left( \theta_2 x_{n}^{2} + \theta_1 x_n + \theta_0 - y_n \right)}}{N \sqrt{\sum_{n=1}^{N}{\left( \theta_2 x_{n}^{2} + \theta_1 x_n + \theta_0 - y_n \right)^2}}}
\end{eqnarray}
$$

# Tópicos avanzados en clasificación
